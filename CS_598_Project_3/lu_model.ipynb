{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-01T00:35:00.510529300Z",
     "start_time": "2023-12-01T00:35:00.506548900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Yangliang\n",
      "[nltk_data]     Lu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download nltk stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T00:35:00.636299200Z",
     "start_time": "2023-12-01T00:35:00.511530100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def read_data(i):\n",
    "\ttrain = pd.read_csv('data/split_' + str(i) + '/train.tsv', sep='\\t')\n",
    "\ttest = pd.read_csv('data/split_' + str(i) + '/test.tsv', sep='\\t')\n",
    "\ttest_y = pd.read_csv('data/split_' + str(i) + '/test_y.tsv', sep='\\t')\n",
    "\n",
    "\ttrain_label = train['sentiment']\n",
    "\ttrain_review =  train['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
    "\n",
    "\ttest_label = test_y['sentiment']\n",
    "\ttest_review = test['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
    "\n",
    "\treturn train_review, train_label, test_review, test_label\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T00:35:00.641481800Z",
     "start_time": "2023-12-01T00:35:00.639299100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "0    supposed comedy\n1       worse acting\n2       please waste\n3          instead 1\n4         forwarding\nName: Feature, dtype: object"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read myvocab.txt\n",
    "vocab = pd.read_csv('myvocab.txt', sep='\\t')\n",
    "vocab['Feature'].head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T00:35:00.682603200Z",
     "start_time": "2023-12-01T00:35:00.641481800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def vectorizer(min_df = None, max_df = None, ngram_range = (1, 2)):\n",
    "    vector = TfidfVectorizer(\n",
    "        preprocessor=lambda x: x.lower(),  # Convert to lowercase\n",
    "        stop_words=stopwords,             # Remove stop words\n",
    "        ngram_range=ngram_range,               # Use 1- to 4-grams\n",
    "          # Use word tokenizer: See Ethan's comment below\n",
    "    )\n",
    "    if min_df is not None:\n",
    "        vector.set_params(min_df=min_df)\n",
    "    if max_df is not None:\n",
    "        vector.set_params(max_df=max_df)\n",
    "    return vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T00:35:00.682603200Z",
     "start_time": "2023-12-01T00:35:00.678588900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pickle import load\n",
    "def fit_vectorizer(train_review, test_review, vocab):\n",
    "\tvector = vectorizer()\n",
    "\t# vector = load(open('vectorizer.pkl', 'rb'))\n",
    "\tvector.fit(vocab)\n",
    "\ttrain = vector.transform(train_review)\n",
    "\ttest = vector.transform(test_review)\n",
    "\treturn train, test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T00:35:00.696376900Z",
     "start_time": "2023-12-01T00:35:00.681096400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "def score(train_review, train_label, test_review, test_label):\n",
    "\t# fit with logistic regression for classification\n",
    "\tfrom sklearn.metrics import accuracy_score\n",
    "\n",
    "\tmodel = LogisticRegressionCV(cv=5, max_iter=10000, n_jobs=-1, solver='liblinear')\n",
    "\tmodel.fit(train_review, train_label)\n",
    "\t# calculate AUC score\n",
    "\tpred = model.predict_proba(test_review)\n",
    "\n",
    "\treturn roc_auc_score(test_label, pred[:, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T00:35:00.697378Z",
     "start_time": "2023-12-01T00:35:00.688903100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def main(i, vocab):\n",
    "\ttrain_review, train_label, test_review, test_label = read_data(i)\n",
    "\n",
    "\ttrain, test = fit_vectorizer(train_review, test_review, vocab['Feature'])\n",
    "\ts = score(train, train_label, test, test_label)\n",
    "\tresult = 'split_' + str(i) + ' ' + str(s) + '\\n'\n",
    "\treturn result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T00:35:00.702015500Z",
     "start_time": "2023-12-01T00:35:00.694377100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# using joblib to parallelize the process\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "count = cpu_count()\n",
    "\n",
    "results = Parallel(n_jobs=count)(delayed(main)(i, vocab) for i in range(1, 6))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T00:35:30.941692100Z",
     "start_time": "2023-12-01T00:35:00.702015500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_1 0.9607600025218678\n",
      "\n",
      "split_2 0.9611106214308613\n",
      "\n",
      "split_3 0.9604750611738162\n",
      "\n",
      "split_4 0.9613678728754387\n",
      "\n",
      "split_5 0.9608693665237437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "\tprint(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T00:35:30.945579300Z",
     "start_time": "2023-12-01T00:35:30.942692900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T00:35:30.947584Z",
     "start_time": "2023-12-01T00:35:30.944515800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
