attr(newX, "scaled:center")
pca1$center
newX = t(t(X) - colMeans(X))
pca1 = prcomp(newX, center = FALSE, scale = FALSE)
pca1$center
junk1 = predict(pca1, newdata = newX)
junk1
pca2 = prcomp(newX, center = TRUE, scale = FALSE)
junk2 = predict(pca2, newdata = newX)
junk2
n = 10; p=5
X = matrix(rnorm(n*p), n, p)
newX = scale(X, center = TRUE, scale = FALSE)
pca = prcomp(newX, center = FALSE, scale = FALSE)
F1 = newX %*% pca$rotation
F2 = predict(pca, newdata = newX)
sum((F1 - F2)^2)
colMeans(newX)
attr(newX, "scaled:center")
pca$center
newF2 = t(t(F2) + pca$center)
sum((F1 - newF2)^2)
newX = scale(X, center = TRUE, scale = FALSE)
pca = prcomp(newX, center = FALSE, scale = FALSE)
F1 = newX %*% pca$rotation
F2 = predict(pca, newdata = newX)
sum((F1 - F2)^2)
colMeans(newX)
attr(newX, "scaled:center")
pca$center
newF2 = t(t(F2) + pca$center)
sum((F1 - newF2)^2)
newF2 = t(t(F2) - pca$center)
sum((F1 - newF2)^2)
newF2 = t(t(F2) - pca$center)
colMeans(newF2)
colMeans(F2)
pca$center
t(t(newX) + pca$center) %*% pca$rotation
F2
t(t(newX) - pca$center) %*% pca$rotation
newF2 = t(t(newX) + pca$center) %*% pca$rotation
sum((F1 - newF2)^2)
newF2 = t(t(newX) - pca$center) %*% pca$rotation
sum((F1 - newF2)^2)
newF2 = t(t(newX) - pca$center) %*% pca$rotation
sum((F2 - newF2)^2)
pca = prcomp(newX, center = TRUE, scale = FALSE)
F1 = newX %*% pca$rotation
F2 = predict(pca, newdata = newX)
sum((F1 - F2)^2)
pca$center
newX = t(t(X) - colMeans(X))
pca = prcomp(newX, center = TRUE, scale = FALSE)
F1 = newX %*% pca$rotation
F2 = predict(pca, newdata = newX)
sum((F1 - F2)^2)
n = 10; p=5
X = matrix(rnorm(n*p), n, p)
newX = scale(X, center = TRUE, scale = FALSE)
pca = prcomp(newX, center = FALSE, scale = FALSE)
F1 = newX %*% pca$rotation
F2 = predict(pca, newdata = newX)
sum((F1 - F2)^2)
colMeans(newX)
attr(newX, "scaled:center")
pca$center
newF2 = t(t(newX) - pca$center) %*% pca$rotation
sum((F2 - newF2)^2)
pca = prcomp(newX, center = TRUE, scale = FALSE)
F1 = newX %*% pca$rotation
F2 = predict(pca, newdata = newX)
sum((F1 - F2)^2)
pca$center
newX = t(t(X) - colMeans(X))
pca = prcomp(newX, center = TRUE, scale = FALSE)
F1 = newX %*% pca$rotation
F2 = predict(pca, newdata = newX)
sum((F1 - F2)^2)
newX = t(t(X) - colMeans(X))
pca = prcomp(newX, center = FALSE, scale = FALSE)
F1 = newX %*% pca$rotation
F2 = predict(pca, newdata = newX)
sum((F1 - F2)^2)
?poly
poly()
poly
library(e1071)
install.packages("e1071")
library(e1071)
spam = read.table(file="https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data")
names(spam)[58] = "Y"
spam$Y = as.factor(spam$Y)
testID = c(1:100, 1901:1960)
spam.test=spam[testID, ];
spam.train=spam[-testID, ];
## Linear SVM
svmfit=svm(Y ~., kernel="linear", data=spam.train, cost=1)
summary(svmfit)
table(spam.train$Y, svmfit$fitted)
svmpred=predict(svmfit, newdata=spam.test)
table(spam.test$Y, svmpred)
## Gaussian kernel SVM
svmfit=svm(Y ~., data=spam.train, cost=1)
summary(svmfit)
svmfit=svm(Y ~., kernel="linear", data=spam.train, cost=10)
summary(svmfit)
table(spam.train$Y, svmfit$fitted)
svmpred=predict(svmfit, newdata=spam.test)
table(spam.test$Y, svmpred)
svmfit=svm(Y ~., kernel="linear", data=spam.train, cost=50)
summary(svmfit)
table(spam.train$Y, svmfit$fitted)
svmpred=predict(svmfit, newdata=spam.test)
table(spam.test$Y, svmpred)
svmfit=svm(Y ~., data=spam.train, cost=1)
summary(svmfit)
table(spam.train$Y, svmfit$fitted)
svmpred=predict(svmfit, newdata=spam.test)
table(spam.test$Y, svmpred)
svmfit=svm(Y ~., data=spam.train, cost=10)
summary(svmfit)
table(spam.train$Y, svmfit$fitted)
svmpred=predict(svmfit, newdata=spam.test)
table(spam.test$Y, svmpred)
svmfit=svm(Y ~., data=spam.train, cost=50)
summary(svmfit)
table(spam.train$Y, svmfit$fitted)
svmpred=predict(svmfit, newdata=spam.test)
table(spam.test$Y, svmpred)
x=sample(1:4, 4)
x
y = sample(1:4,3)
y
outer(x,y)
x %*% y
as.vector(x)
t(as.vector(x)) %*% as.vector(y)
as.matrix(x, 4, 1) %*% as.matrix(y, 1, 3)
as.matrix(x, 4, 1)
as.matrix(y, 1, 3)
as.matrix(x, 4, 1) %*% y
?glm
x = rnorm(10); y = sample(c(0, 1), 10)
x = rnorm(10); y = sample(c(0, 1), 10, replace=TRUE)
tmp = glm(y ~ x, family="bionomial")
tmp = glm(y ~ x, family="binomial")
tmp$fitted.values
predict(tmp, x)
data = data.frame(x = x, y = y)
tmp = glm(y ~ x, family="binomial", data)
tmp$fitted.values
predict(tmp, data=data)
predict(tmp, data=data, type="response")
source("~/Box Sync/Dolphin/Course/datamining/f22/Coding/F22_Coding4_Part_I.Rmd")
source("~/Box Sync/Dolphin/Course/datamining/f22/Coding/F22_Coding4_Part_I.Rmd")
source("~/Box Sync/Dolphin/Course/datamining/f22/Coding/F22_Coding4_Part_I.Rmd")
source("~/Box Sync/Dolphin/Course/datamining/f22/Coding/F22_Coding4_Part_I.Rmd")
source("~/Box Sync/Dolphin/Course/datamining/f22/Coding/F22_Coding4_Part_I.Rmd")
source("~/Box Sync/Dolphin/Course/datamining/f22/Coding/F22_Coding4_Part_I.Rmd")
source("~/Box Sync/Dolphin/Course/datamining/f22/Coding/F22_Coding4_Part_I.Rmd")
source("~/Box Sync/Dolphin/Course/datamining/f22/Coding/F22_Coding4_Part_I.Rmd")
source("~/Box Sync/Dolphin/Course/datamining/f22/Coding/F22_Coding4_Part_I.Rmd")
source("~/Box Sync/Dolphin/Course/datamining/f22/Coding/F22_Coding4_Part_I.Rmd")
2+(3*2)
exp(0)
sin(2)
1/2
sqrt(3)
3>2  ## returns False or True
x <- c(1,3,2,5)
x
x = c(1,6,2)
x
y = c(1,4,3)
length(x)
length(y)
?length          ## Check the help page for command "length"
x+y
ls()            ## Display what objects are in your current R session
## You can also just click "Environment" in one of the panel
rm(x,y)         ## remove the two objects
ls()
rm(list=ls())   ## remove all objects in the current R session
?matrix
x=matrix(data=c(1,2,3,4,5,6), nrow=3, ncol=2)
x
x + c(1,2)      ## How does R compute a matrix plus a vector?
x + c(1,2,3,4)  ## error message
matrix(1:6,3,2,byrow=TRUE)
x
sqrt(x)
x^2
rnorm(5)  # normal random variables with mean zero and variance 1
runif(5)  # random samples from interval (0, 1)
rnorm(2)
rnorm(2)
set.seed(1303); rnorm(2)
set.seed(1303); rnorm(2)
set.seed(3); rnorm(2)
set.seed(3); rnorm(2)
y=rnorm(100)
mean(y)
var(y)
sqrt(var(y))  ## same as sd(y)
sd(y)
sum(y)/len(y)
y=rnorm(100)
y
mean(y)
sum(y)/length(y)
var(y)
sqrt(var(y))  ## same as sd(y)
sd(y)
y=rnorm(10)
y
mean(y)
sum(y)/length(y)
var(y)
sqrt(var(y))  ## same as sd(y)
sd(y)
x=rnorm(100)
y=rnorm(100)
plot(x,y)
plot(x,y,xlab="this is the x-axis",ylab="this is the y-axis",main="Plot of X vs Y")
x=seq(1,10)
x
x=1:10
x
x=seq(-pi,pi,length=50)
y=x
f=outer(x,y,function(x,y)cos(y)/(1+x^2))
contour(x,y,f)
contour(x,y,f,nlevels=45,add=T)
fa=(f-t(f))/2
contour(x,y,fa,nlevels=15)
image(x,y,fa)
persp(x,y,fa)
persp(x,y,fa,theta=30)
x=seq(1,10)
x
x=1:10
x
x=seq(-pi,pi,length=50)
y=x
f=outer(x,y,function(x,y)cos(y)/(1+x^2))
contour(x,y,f)
contour(x,y,f,nlevels=45,add=T)
fa=(f-t(f))/2
contour(x,y,fa,nlevels=15)
image(x,y,fa)
persp(x,y,fa)
persp(x,y,fa,theta=30)
persp(x,y,fa,theta=30,phi=20)
persp(x,y,fa,theta=30,phi=70)
persp(x,y,fa,theta=30,phi=40)
x=seq(1,10)
x
x=1:10
x
x=seq(-pi,pi,length=50)
y=x
f=outer(x,y,function(x,y)cos(y)/(1+x^2))
contour(x,y,f)
contour(x,y,f,nlevels=45,add=T)
fa=(f-t(f))/2
contour(x,y,fa,nlevels=15)
image(x,y,fa)
Auto=read.table("http://www-bcf.usc.edu/~gareth/ISL/Auto.data")
Auto=read.table("http://www-bcf.usc.edu/~gareth/ISL/Auto.data",header=T)
Auto=read.table("https://www.statlearning.com/s/Auto.data")
head(Auto)               ## show the first 5 rows of the data
Auto=read.table("https://www.statlearning.com/s/Auto.data",header=T)
head(Auto)
str(Auto)               ## Show structural info of "Auto"
dim(Auto)
Auto[1:4,]
names(Auto)
Auto[1:4,1]
Auto$mpg[1:4]
summary(Auto)
unique(Auto$cylinders)
unique(Auto$year)
unique(Auto$origin)
table(Auto$cylinders)             ## frequency tables
table(Auto$year)
table(Auto$origin)
plot(cylinders, mpg)
# Attach a data matrix, so each column can be accessed by its name
attach(Auto)
plot(cylinders, mpg)
cylinders=as.factor(cylinders)
plot(cylinders, mpg)
plot(cylinders, mpg, col="red")
plot(cylinders, mpg, col="red", varwidth=T)
plot(cylinders, mpg, col="red", varwidth=T,horizontal=T)
plot(cylinders, mpg, col="red", varwidth=T, xlab="cylinders", ylab="MPG")
## Histograms
hist(mpg)
hist(mpg,col=2)
hist(mpg,col=2,breaks=15)
## All pairwise plots between two variables
pairs(Auto)
pairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)
plot(horsepower,mpg)
# Attach a data matrix, so each column can be accessed by its name
attach(Auto)
plot(cylinders, mpg)
cylinders=as.factor(cylinders)
plot(cylinders, mpg)
plot(cylinders, mpg, col="red")
plot(cylinders, mpg, col="red", varwidth=T)
plot(cylinders, mpg, col="red", varwidth=T,horizontal=T)
plot(cylinders, mpg, col="red", varwidth=T, xlab="cylinders", ylab="MPG")
## Histograms
hist(mpg)
hist(mpg,col=2)
hist(mpg,col=2,breaks=15)
plot(horsepower,mpg)
# Attach a data matrix, so each column can be accessed by its name
attach(Auto)
plot(cylinders, mpg)
cylinders = as.factor(cylinders)
plot(cylinders, mpg)
plot(cylinders, mpg, col="red")
plot(cylinders, mpg, col="red", varwidth = T)
plot(cylinders, mpg, col="red", varwidth = T, horizontal = T)
plot(cylinders, mpg, col="red", varwidth = T,
xlab = "cylinders", ylab = "MPG")
## Histograms
hist(mpg)
hist(mpg,col = 2)
hist(mpg,col = 2, breaks = 15)
n = 20; y = rnorm(20); x = c(rep(2010, 7), rep(2011, 6), rep(2012, 7))
x = x - 2011
x
summary(lm(y ~ x + I(x^2)))
x = x + 2011
x
summary(lm(y ~ x + I(x^2)))
3 + 2
4^2
36*2.2
diag(c(1, 2, 3))
setwd("~/mydoc/Course/datamining/Project/Sentiment/")
data <- read.table("alldata.tsv", stringsAsFactors = FALSE,
header = TRUE)
testIDs <- read.csv("project3_splits.csv", header = TRUE)
for(j in 1:5){
dir.create(paste("split_", j, sep=""))
train <- data[-testIDs[,j], c("id", "sentiment", "review") ]
test <- data[testIDs[,j], c("id", "review")]
test.y <- data[testIDs[,j], c("id", "sentiment", "score")]
tmp_file_name <- paste("split_", j, "/", "train.tsv", sep="")
write.table(train, file=tmp_file_name,
quote=TRUE,
row.names = FALSE,
sep='\t')
tmp_file_name <- paste("split_", j, "/", "test.tsv", sep="")
write.table(test, file=tmp_file_name,
quote=TRUE,
row.names = FALSE,
sep='\t')
tmp_file_name <- paste("split_", j, "/", "test_y.tsv", sep="")
write.table(test.y, file=tmp_file_name,
quote=TRUE,
row.names = FALSE,
sep='\t')
}
if (!require("pacman")) install.packages("pacman")
pacman::p_load("pROC",
"text2vec",
"slam")
j = 1
setwd(paste("split_", j, sep=""))
train = read.table("train.tsv",
stringsAsFactors = FALSE,
header = TRUE)
train$review = gsub('<.*?>', ' ', train$review)
ls()
dim(train)
train[1, ]
stop_words = c("i", "me", "my", "myself",
"we", "our", "ours", "ourselves",
"you", "your", "yours",
"their", "they", "his", "her",
"she", "he", "a", "an", "and",
"is", "was", "are", "were",
"him", "himself", "has", "have",
"it", "its", "the", "us")
it_train = itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
search()
libary(text2vec)
library(text2vec)
library("text2vec")
install.packages(text2vec)
library(text2vec)
install.packages(text2vec)
install.packages("text2vec")
install.packages("pROC")
install.packages("naivebayes")
library(text2vec)
it_train = itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
summary(it_train)
attributes(it_train)
tmp.vocab = create_vocabulary(it_train,
stopwords = stop_words,
ngram = c(1L,4L))
tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10,
doc_proportion_max = 0.5,
doc_proportion_min = 0.001)
dtm_train  = create_dtm(it_train, vocab_vectorizer(tmp.vocab))
dim(dtm_train)
set.seed(2022)
tmpfit = glmnet(x = dtm_train,
y = train$sentiment,
alpha = 1,
family='binomial')
library(glmnet)
set.seed(2022)
tmpfit = glmnet(x = dtm_train,
y = train$sentiment,
alpha = 1,
family='binomial')
tmpfit$df
names(dtm_train)
dtm_train[1, ]
dtm_train[, 1]
range(dtm_train[, 1])
range(dtm_train[, 2])
range(dtm_train[, 4])
range(dtm_train)
tmpfit$df
myvocab = colnames(dtm_train)[which(tmpfit$beta[, 68] != 0)]
myvocab
length(mycovcab)
length(myvocab)
tmpfit$df
myvocab = colnames(dtm_train)[which(tmpfit$beta[, 78] != 0)]
sort(myvocab)
v.size = dim(dtm_train)[2]
ytrain = train$sentiment
summ = matrix(0, nrow=v.size, ncol=4)
summ[,1] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==1, ]), mean)
library(slam)
summ = matrix(0, nrow=v.size, ncol=4)
summ[,1] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==1, ]), mean)
install.packages("slam")
summ = matrix(0, nrow=v.size, ncol=4)
summ[,1] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==1, ]), mean)
library(slam)
summ = matrix(0, nrow=v.size, ncol=4)
summ[,1] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==1, ]), mean)
summ[,2] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==1, ]), var)
summ[,3] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==0, ]), mean)
summ[,4] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==0, ]), var)
dim(summ)
sum(summ[, 2] == 0)
sum(summ[, 4] == 0)
which(summ[, 2] == 0)
colnames(dtm_train)
words = colnames(dtm_train)
id1 = which(summ[, 2] == 0)
id0 = which(summ[, 4] == 0)
which(summ[id1, 1] != 0)
which(summ[id0, 3] != 0)
which(summ[id1, 3] != 0)
junk = which(summ[, 1] == 0)
cbind(junk, id1)
interaction(id1, id0)
intersect(id1, id0)
words = colnames(dtm_train)
words[id1]
words[id0]
n1 = sum(ytrain);
n = length(ytrain)
n0 = n - n1
myp = (summ[,1] - summ[,3])/
sqrt(summ[,2]/n1 + summ[,4]/n0)
pos.list = words[id[myp[id]>0]]
neg.list = words[id[myp[id]<0]]
id = order(abs(myp), decreasing=TRUE)[1:2000]
pos.list = words[id[myp[id]>0]]
neg.list = words[id[myp[id]<0]]
intersect(id, id0)
words[intersect(id, id0)]
words[intersect(id, id1)]
all(id0 %in% id)
words[id0]
words[intersect(id0, id)]
all(id1 %in% id)
id0[intersect(id0, id)]
id0[id0 %in% id == TRUE]
id0 %in% id
id0
words[id0[! (id0 %in% id)]]
words[id1[! (id1 %in% id)]]
pos.list
neg.list
pos.list
pos.list[1:50]
